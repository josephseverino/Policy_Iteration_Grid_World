{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://deeplearningcourses.com/c/artificial-intelligence-reinforcement-learning-in-python\n",
    "# https://www.udemy.com/artificial-intelligence-reinforcement-learning-in-python\n",
    "from __future__ import print_function, division\n",
    "from builtins import range\n",
    "# Note: you may need to update your version of future\n",
    "# sudo pip install -U future\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Grid: # Environment\n",
    "  def __init__(self, width, height, start):\n",
    "    self.width = width\n",
    "    self.height = height\n",
    "    self.i = start[0]\n",
    "    self.j = start[1]\n",
    "\n",
    "  def set(self, rewards, actions):\n",
    "    # rewards should be a dict of: (i, j): r (row, col): reward\n",
    "    # actions should be a dict of: (i, j): A (row, col): list of possible actions\n",
    "    self.rewards = rewards\n",
    "    self.actions = actions\n",
    "\n",
    "  def set_state(self, s):\n",
    "    self.i = s[0]\n",
    "    self.j = s[1]\n",
    "\n",
    "  def current_state(self):\n",
    "    return (self.i, self.j)\n",
    "\n",
    "  def is_terminal(self, s):\n",
    "    return s not in self.actions\n",
    "\n",
    "  def move(self, action):\n",
    "    # check if legal move first\n",
    "    if action in self.actions[(self.i, self.j)]:\n",
    "      if action == 'U':\n",
    "        self.i -= 1\n",
    "      elif action == 'D':\n",
    "        self.i += 1\n",
    "      elif action == 'R':\n",
    "        self.j += 1\n",
    "      elif action == 'L':\n",
    "        self.j -= 1\n",
    "    # return a reward (if any)\n",
    "    return self.rewards.get((self.i, self.j), 0)\n",
    "\n",
    "  def undo_move(self, action):\n",
    "    # these are the opposite of what U/D/L/R should normally do\n",
    "    if action == 'U':\n",
    "      self.i += 1\n",
    "    elif action == 'D':\n",
    "      self.i -= 1\n",
    "    elif action == 'R':\n",
    "      self.j -= 1\n",
    "    elif action == 'L':\n",
    "      self.j += 1\n",
    "    # raise an exception if we arrive somewhere we shouldn't be\n",
    "    # should never happen\n",
    "    assert(self.current_state() in self.all_states())\n",
    "\n",
    "  def game_over(self):\n",
    "    # returns true if game is over, else false\n",
    "    # true if we are in a state where no actions are possible\n",
    "    return (self.i, self.j) not in self.actions\n",
    "\n",
    "  def all_states(self):\n",
    "    # possibly buggy but simple way to get all states\n",
    "    # either a position that has possible next actions\n",
    "    # or a position that yields a reward\n",
    "    return set(self.actions.keys()) | set(self.rewards.keys())\n",
    "\n",
    "\n",
    "def standard_grid():\n",
    "  # define a grid that describes the reward for arriving at each state\n",
    "  # and possible actions at each state\n",
    "  # the grid looks like this\n",
    "  # x means you can't go there\n",
    "  # s means start position\n",
    "  # number means reward at that state\n",
    "  # .  .  .  1\n",
    "  # .  x  . -1\n",
    "  # s  .  .  .\n",
    "  g = Grid(3, 4, (2, 0))\n",
    "  rewards = {(0, 3): 1, (1, 3): -1}\n",
    "  actions = {\n",
    "    (0, 0): ('D', 'R'),\n",
    "    (0, 1): ('L', 'R'),\n",
    "    (0, 2): ('L', 'D', 'R'),\n",
    "    (1, 0): ('U', 'D'),\n",
    "    (1, 2): ('U', 'D', 'R'),\n",
    "    (2, 0): ('U', 'R'),\n",
    "    (2, 1): ('L', 'R'),\n",
    "    (2, 2): ('L', 'R', 'U'),\n",
    "    (2, 3): ('L', 'U'),\n",
    "  }\n",
    "  g.set(rewards, actions)\n",
    "  return g\n",
    "\n",
    "\n",
    "def negative_grid(step_cost=-0.1):\n",
    "  # in this game we want to try to minimize the number of moves\n",
    "  # so we will penalize every move\n",
    "  g = standard_grid()\n",
    "  g.rewards.update({\n",
    "    (0, 0): step_cost,\n",
    "    (0, 1): step_cost,\n",
    "    (0, 2): step_cost,\n",
    "    (1, 0): step_cost,\n",
    "    (1, 2): step_cost,\n",
    "    (2, 0): step_cost,\n",
    "    (2, 1): step_cost,\n",
    "    (2, 2): step_cost,\n",
    "    (2, 3): step_cost,\n",
    "  })\n",
    "  return g\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.5 new_v 0.0 V: 0 Reward(r): 0 State: (0, 1) action: L\n",
      "1 0.5 new_v 0.0 V: 0 Reward(r): 0 State: (0, 1) action: R\n",
      "2 0.3333333333333333 new_v 0.0 V: 0 Reward(r): 0 State: (1, 2) action: U\n",
      "2 0.3333333333333333 new_v 0.0 V: 0 Reward(r): 0 State: (1, 2) action: D\n",
      "2 0.3333333333333333 new_v -0.3333333333333333 V: 0 Reward(r): -1 State: (1, 2) action: R\n",
      "3 0.5 new_v 0.0 V: 0 Reward(r): 0 State: (0, 0) action: D\n",
      "3 0.5 new_v 0.0 V: 0.0 Reward(r): 0 State: (0, 0) action: R\n",
      "5 0.5 new_v 0.0 V: 0 Reward(r): 0 State: (2, 1) action: L\n",
      "5 0.5 new_v 0.0 V: 0 Reward(r): 0 State: (2, 1) action: R\n",
      "6 0.5 new_v 0.0 V: 0 Reward(r): 0 State: (2, 0) action: U\n",
      "6 0.5 new_v 0.0 V: 0.0 Reward(r): 0 State: (2, 0) action: R\n",
      "7 0.5 new_v 0.0 V: 0 Reward(r): 0 State: (2, 3) action: L\n",
      "7 0.5 new_v -0.5 V: 0 Reward(r): -1 State: (2, 3) action: U\n",
      "8 0.3333333333333333 new_v 0.0 V: 0.0 Reward(r): 0 State: (2, 2) action: L\n",
      "8 0.3333333333333333 new_v -0.15 V: -0.5 Reward(r): 0 State: (2, 2) action: R\n",
      "8 0.3333333333333333 new_v -0.25 V: -0.3333333333333333 Reward(r): 0 State: (2, 2) action: U\n",
      "9 0.5 new_v 0.0 V: 0.0 Reward(r): 0 State: (1, 0) action: U\n",
      "9 0.5 new_v 0.0 V: 0.0 Reward(r): 0 State: (1, 0) action: D\n",
      "10 0.3333333333333333 new_v 0.0 V: 0.0 Reward(r): 0 State: (0, 2) action: L\n",
      "10 0.3333333333333333 new_v -0.09999999999999999 V: -0.3333333333333333 Reward(r): 0 State: (0, 2) action: D\n",
      "10 0.3333333333333333 new_v 0.23333333333333334 V: 0 Reward(r): 1 State: (0, 2) action: R\n",
      "12 0.5 new_v 0.0 V: 0.0 Reward(r): 0 State: (0, 1) action: L\n",
      "12 0.5 new_v 0.10500000000000001 V: 0.23333333333333334 Reward(r): 0 State: (0, 1) action: R\n",
      "13 0.3333333333333333 new_v 0.07 V: 0.23333333333333334 Reward(r): 0 State: (1, 2) action: U\n",
      "13 0.3333333333333333 new_v -0.0049999999999999906 V: -0.25 Reward(r): 0 State: (1, 2) action: D\n",
      "13 0.3333333333333333 new_v -0.3383333333333333 V: 0 Reward(r): -1 State: (1, 2) action: R\n",
      "14 0.5 new_v 0.0 V: 0.0 Reward(r): 0 State: (0, 0) action: D\n",
      "14 0.5 new_v 0.04725000000000001 V: 0.10500000000000001 Reward(r): 0 State: (0, 0) action: R\n",
      "16 0.5 new_v 0.0 V: 0.0 Reward(r): 0 State: (2, 1) action: L\n",
      "16 0.5 new_v -0.1125 V: -0.25 Reward(r): 0 State: (2, 1) action: R\n",
      "17 0.5 new_v 0.0 V: 0.0 Reward(r): 0 State: (2, 0) action: U\n",
      "17 0.5 new_v -0.050625 V: -0.1125 Reward(r): 0 State: (2, 0) action: R\n",
      "18 0.5 new_v -0.1125 V: -0.25 Reward(r): 0 State: (2, 3) action: L\n",
      "18 0.5 new_v -0.6125 V: 0 Reward(r): -1 State: (2, 3) action: U\n",
      "19 0.3333333333333333 new_v -0.03375 V: -0.1125 Reward(r): 0 State: (2, 2) action: L\n",
      "19 0.3333333333333333 new_v -0.2175 V: -0.6125 Reward(r): 0 State: (2, 2) action: R\n",
      "19 0.3333333333333333 new_v -0.319 V: -0.3383333333333333 Reward(r): 0 State: (2, 2) action: U\n",
      "20 0.5 new_v 0.021262500000000004 V: 0.04725000000000001 Reward(r): 0 State: (1, 0) action: U\n",
      "20 0.5 new_v -0.0015187499999999993 V: -0.050625 Reward(r): 0 State: (1, 0) action: D\n",
      "21 0.3333333333333333 new_v 0.0315 V: 0.10500000000000001 Reward(r): 0 State: (0, 2) action: L\n",
      "21 0.3333333333333333 new_v -0.06999999999999999 V: -0.3383333333333333 Reward(r): 0 State: (0, 2) action: D\n",
      "21 0.3333333333333333 new_v 0.2633333333333333 V: 0 Reward(r): 1 State: (0, 2) action: R\n",
      "23 0.5 new_v 0.021262500000000004 V: 0.04725000000000001 Reward(r): 0 State: (0, 1) action: L\n",
      "23 0.5 new_v 0.1397625 V: 0.2633333333333333 Reward(r): 0 State: (0, 1) action: R\n",
      "24 0.3333333333333333 new_v 0.07899999999999999 V: 0.2633333333333333 Reward(r): 0 State: (1, 2) action: U\n",
      "24 0.3333333333333333 new_v -0.01670000000000002 V: -0.319 Reward(r): 0 State: (1, 2) action: D\n",
      "24 0.3333333333333333 new_v -0.3500333333333333 V: 0 Reward(r): -1 State: (1, 2) action: R\n",
      "25 0.5 new_v -0.0006834374999999997 V: -0.0015187499999999993 Reward(r): 0 State: (0, 0) action: D\n",
      "25 0.5 new_v 0.062209687500000006 V: 0.1397625 Reward(r): 0 State: (0, 0) action: R\n",
      "27 0.5 new_v -0.022781250000000003 V: -0.050625 Reward(r): 0 State: (2, 1) action: L\n",
      "27 0.5 new_v -0.16633125 V: -0.319 Reward(r): 0 State: (2, 1) action: R\n",
      "28 0.5 new_v -0.0006834374999999997 V: -0.0015187499999999993 Reward(r): 0 State: (2, 0) action: U\n",
      "28 0.5 new_v -0.0755325 V: -0.16633125 Reward(r): 0 State: (2, 0) action: R\n",
      "29 0.5 new_v -0.14355 V: -0.319 Reward(r): 0 State: (2, 3) action: L\n",
      "29 0.5 new_v -0.6435500000000001 V: 0 Reward(r): -1 State: (2, 3) action: U\n",
      "30 0.3333333333333333 new_v -0.049899375 V: -0.16633125 Reward(r): 0 State: (2, 2) action: L\n",
      "30 0.3333333333333333 new_v -0.24296437500000004 V: -0.6435500000000001 Reward(r): 0 State: (2, 2) action: R\n",
      "30 0.3333333333333333 new_v -0.34797437500000006 V: -0.3500333333333333 Reward(r): 0 State: (2, 2) action: U\n",
      "31 0.5 new_v 0.027994359375000003 V: 0.062209687500000006 Reward(r): 0 State: (1, 0) action: U\n",
      "31 0.5 new_v -0.005995265624999999 V: -0.0755325 Reward(r): 0 State: (1, 0) action: D\n",
      "32 0.3333333333333333 new_v 0.04192875 V: 0.1397625 Reward(r): 0 State: (0, 2) action: L\n",
      "32 0.3333333333333333 new_v -0.06308124999999999 V: -0.3500333333333333 Reward(r): 0 State: (0, 2) action: D\n",
      "32 0.3333333333333333 new_v 0.27025208333333334 V: 0 Reward(r): 1 State: (0, 2) action: R\n",
      "34 0.5 new_v 0.027994359375000003 V: 0.062209687500000006 Reward(r): 0 State: (0, 1) action: L\n",
      "34 0.5 new_v 0.149607796875 V: 0.27025208333333334 Reward(r): 0 State: (0, 1) action: R\n",
      "35 0.3333333333333333 new_v 0.081075625 V: 0.27025208333333334 Reward(r): 0 State: (1, 2) action: U\n",
      "35 0.3333333333333333 new_v -0.02331668750000003 V: -0.34797437500000006 Reward(r): 0 State: (1, 2) action: D\n",
      "35 0.3333333333333333 new_v -0.35665002083333336 V: 0 Reward(r): -1 State: (1, 2) action: R\n",
      "36 0.5 new_v -0.0026978695312499996 V: -0.005995265624999999 Reward(r): 0 State: (0, 0) action: D\n",
      "36 0.5 new_v 0.0646256390625 V: 0.149607796875 Reward(r): 0 State: (0, 0) action: R\n",
      "38 0.5 new_v -0.033989625 V: -0.0755325 Reward(r): 0 State: (2, 1) action: L\n",
      "38 0.5 new_v -0.19057809375000004 V: -0.34797437500000006 Reward(r): 0 State: (2, 1) action: R\n",
      "39 0.5 new_v -0.0026978695312499996 V: -0.005995265624999999 Reward(r): 0 State: (2, 0) action: U\n",
      "39 0.5 new_v -0.08845801171875001 V: -0.19057809375000004 Reward(r): 0 State: (2, 0) action: R\n",
      "40 0.5 new_v -0.15658846875000004 V: -0.34797437500000006 Reward(r): 0 State: (2, 3) action: L\n",
      "40 0.5 new_v -0.6565884687500001 V: 0 Reward(r): -1 State: (2, 3) action: U\n",
      "41 0.3333333333333333 new_v -0.05717342812500001 V: -0.19057809375000004 Reward(r): 0 State: (2, 2) action: L\n",
      "41 0.3333333333333333 new_v -0.25414996875000007 V: -0.6565884687500001 Reward(r): 0 State: (2, 2) action: R\n",
      "41 0.3333333333333333 new_v -0.3611449750000001 V: -0.35665002083333336 Reward(r): 0 State: (2, 2) action: U\n",
      "42 0.5 new_v 0.029081537578125004 V: 0.0646256390625 Reward(r): 0 State: (1, 0) action: U\n",
      "42 0.5 new_v -0.0107245676953125 V: -0.08845801171875001 Reward(r): 0 State: (1, 0) action: D\n",
      "43 0.3333333333333333 new_v 0.0448823390625 V: 0.149607796875 Reward(r): 0 State: (0, 2) action: L\n",
      "43 0.3333333333333333 new_v -0.0621126671875 V: -0.35665002083333336 Reward(r): 0 State: (0, 2) action: D\n",
      "43 0.3333333333333333 new_v 0.2712206661458333 V: 0 Reward(r): 1 State: (0, 2) action: R\n",
      "45 0.5 new_v 0.029081537578125004 V: 0.0646256390625 Reward(r): 0 State: (0, 1) action: L\n",
      "45 0.5 new_v 0.15113083734375 V: 0.2712206661458333 Reward(r): 0 State: (0, 1) action: R\n",
      "46 0.3333333333333333 new_v 0.08136619984374999 V: 0.2712206661458333 Reward(r): 0 State: (1, 2) action: U\n",
      "46 0.3333333333333333 new_v -0.02697729265625004 V: -0.3611449750000001 Reward(r): 0 State: (1, 2) action: D\n",
      "46 0.3333333333333333 new_v -0.36031062598958336 V: 0 Reward(r): -1 State: (1, 2) action: R\n",
      "47 0.5 new_v -0.004826055462890625 V: -0.0107245676953125 Reward(r): 0 State: (0, 0) action: D\n",
      "47 0.5 new_v 0.06318282134179687 V: 0.15113083734375 Reward(r): 0 State: (0, 0) action: R\n",
      "49 0.5 new_v -0.0398061052734375 V: -0.08845801171875001 Reward(r): 0 State: (2, 1) action: L\n",
      "49 0.5 new_v -0.20232134402343754 V: -0.3611449750000001 Reward(r): 0 State: (2, 1) action: R\n",
      "50 0.5 new_v -0.004826055462890625 V: -0.0107245676953125 Reward(r): 0 State: (2, 0) action: U\n",
      "50 0.5 new_v -0.09587066027343752 V: -0.20232134402343754 Reward(r): 0 State: (2, 0) action: R\n",
      "51 0.5 new_v -0.16251523875000004 V: -0.3611449750000001 Reward(r): 0 State: (2, 3) action: L\n",
      "51 0.5 new_v -0.6625152387500001 V: 0 Reward(r): -1 State: (2, 3) action: U\n",
      "52 0.3333333333333333 new_v -0.06069640320703126 V: -0.20232134402343754 Reward(r): 0 State: (2, 2) action: L\n",
      "52 0.3333333333333333 new_v -0.2594509748320313 V: -0.6625152387500001 Reward(r): 0 State: (2, 2) action: R\n",
      "52 0.3333333333333333 new_v -0.36754416262890627 V: -0.36031062598958336 Reward(r): 0 State: (2, 2) action: U\n",
      "53 0.5 new_v 0.028432269603808593 V: 0.06318282134179687 Reward(r): 0 State: (1, 0) action: U\n",
      "53 0.5 new_v -0.014709527519238295 V: -0.09587066027343752 Reward(r): 0 State: (1, 0) action: D\n",
      "54 0.3333333333333333 new_v 0.04533925120312499 V: 0.15113083734375 Reward(r): 0 State: (0, 2) action: L\n",
      "54 0.3333333333333333 new_v -0.06275393659375 V: -0.36031062598958336 Reward(r): 0 State: (0, 2) action: D\n",
      "54 0.3333333333333333 new_v 0.2705793967395833 V: 0 Reward(r): 1 State: (0, 2) action: R\n",
      "56 0.5 new_v 0.028432269603808593 V: 0.06318282134179687 Reward(r): 0 State: (0, 1) action: L\n",
      "56 0.5 new_v 0.15019299813662107 V: 0.2705793967395833 Reward(r): 0 State: (0, 1) action: R\n",
      "57 0.3333333333333333 new_v 0.08117381902187498 V: 0.2705793967395833 Reward(r): 0 State: (1, 2) action: U\n",
      "57 0.3333333333333333 new_v -0.029089429766796904 V: -0.36754416262890627 Reward(r): 0 State: (1, 2) action: D\n",
      "57 0.3333333333333333 new_v -0.3624227631001302 V: 0 Reward(r): -1 State: (1, 2) action: R\n",
      "58 0.5 new_v -0.0066192873836572325 V: -0.014709527519238295 Reward(r): 0 State: (0, 0) action: D\n",
      "58 0.5 new_v 0.060967561777822254 V: 0.15019299813662107 Reward(r): 0 State: (0, 0) action: R\n",
      "60 0.5 new_v -0.04314179712304689 V: -0.09587066027343752 Reward(r): 0 State: (2, 1) action: L\n",
      "60 0.5 new_v -0.20853667030605472 V: -0.36754416262890627 Reward(r): 0 State: (2, 1) action: R\n",
      "61 0.5 new_v -0.0066192873836572325 V: -0.014709527519238295 Reward(r): 0 State: (2, 0) action: U\n",
      "61 0.5 new_v -0.10046078902138186 V: -0.20853667030605472 Reward(r): 0 State: (2, 0) action: R\n",
      "62 0.5 new_v -0.16539487318300783 V: -0.36754416262890627 Reward(r): 0 State: (2, 3) action: L\n",
      "62 0.5 new_v -0.6653948731830078 V: 0 Reward(r): -1 State: (2, 3) action: U\n",
      "63 0.3333333333333333 new_v -0.06256100109181642 V: -0.20853667030605472 Reward(r): 0 State: (2, 2) action: L\n",
      "63 0.3333333333333333 new_v -0.2621794630467188 V: -0.6653948731830078 Reward(r): 0 State: (2, 2) action: R\n",
      "63 0.3333333333333333 new_v -0.37090629197675784 V: -0.3624227631001302 Reward(r): 0 State: (2, 2) action: U\n",
      "64 0.5 new_v 0.027435402800020017 V: 0.060967561777822254 Reward(r): 0 State: (1, 0) action: U\n",
      "64 0.5 new_v -0.017771952259601823 V: -0.10046078902138186 Reward(r): 0 State: (1, 0) action: D\n",
      "65 0.3333333333333333 new_v 0.04505789944098632 V: 0.15019299813662107 Reward(r): 0 State: (0, 2) action: L\n",
      "65 0.3333333333333333 new_v -0.06366892948905273 V: -0.3624227631001302 Reward(r): 0 State: (0, 2) action: D\n",
      "65 0.3333333333333333 new_v 0.2696644038442806 V: 0 Reward(r): 1 State: (0, 2) action: R\n",
      "67 0.5 new_v 0.027435402800020017 V: 0.060967561777822254 Reward(r): 0 State: (0, 1) action: L\n",
      "67 0.5 new_v 0.14878438452994627 V: 0.2696644038442806 Reward(r): 0 State: (0, 1) action: R\n",
      "68 0.3333333333333333 new_v 0.08089932115328417 V: 0.2696644038442806 Reward(r): 0 State: (1, 2) action: U\n",
      "68 0.3333333333333333 new_v -0.03037256643974319 V: -0.37090629197675784 Reward(r): 0 State: (1, 2) action: D\n",
      "68 0.3333333333333333 new_v -0.3637058997730765 V: 0 Reward(r): -1 State: (1, 2) action: R\n",
      "69 0.5 new_v -0.00799737851682082 V: -0.017771952259601823 Reward(r): 0 State: (0, 0) action: D\n",
      "69 0.5 new_v 0.058955594521655 V: 0.14878438452994627 Reward(r): 0 State: (0, 0) action: R\n",
      "71 0.5 new_v -0.04520735505962184 V: -0.10046078902138186 Reward(r): 0 State: (2, 1) action: L\n",
      "71 0.5 new_v -0.21211518644916288 V: -0.37090629197675784 Reward(r): 0 State: (2, 1) action: R\n",
      "72 0.5 new_v -0.00799737851682082 V: -0.017771952259601823 Reward(r): 0 State: (2, 0) action: U\n",
      "72 0.5 new_v -0.10344921241894413 V: -0.21211518644916288 Reward(r): 0 State: (2, 0) action: R\n",
      "73 0.5 new_v -0.16690783138954104 V: -0.37090629197675784 Reward(r): 0 State: (2, 3) action: L\n",
      "73 0.5 new_v -0.666907831389541 V: 0 Reward(r): -1 State: (2, 3) action: U\n",
      "74 0.3333333333333333 new_v -0.06363455593474887 V: -0.21211518644916288 Reward(r): 0 State: (2, 2) action: L\n",
      "74 0.3333333333333333 new_v -0.2637069053516112 V: -0.666907831389541 Reward(r): 0 State: (2, 2) action: R\n",
      "74 0.3333333333333333 new_v -0.37281867528353413 V: -0.3637058997730765 Reward(r): 0 State: (2, 2) action: U\n",
      "75 0.5 new_v 0.026530017534744748 V: 0.058955594521655 Reward(r): 0 State: (1, 0) action: U\n",
      "75 0.5 new_v -0.02002212805378011 V: -0.10344921241894413 Reward(r): 0 State: (1, 0) action: D\n",
      "76 0.3333333333333333 new_v 0.04463531535898388 V: 0.14878438452994627 Reward(r): 0 State: (0, 2) action: L\n",
      "76 0.3333333333333333 new_v -0.06447645457293907 V: -0.3637058997730765 Reward(r): 0 State: (0, 2) action: D\n",
      "76 0.3333333333333333 new_v 0.2688568787603942 V: 0 Reward(r): 1 State: (0, 2) action: R\n",
      "78 0.5 new_v 0.026530017534744748 V: 0.058955594521655 Reward(r): 0 State: (0, 1) action: L\n",
      "78 0.5 new_v 0.14751561297692214 V: 0.2688568787603942 Reward(r): 0 State: (0, 1) action: R\n",
      "79 0.3333333333333333 new_v 0.08065706362811825 V: 0.2688568787603942 Reward(r): 0 State: (1, 2) action: U\n",
      "79 0.3333333333333333 new_v -0.031188538956941986 V: -0.37281867528353413 Reward(r): 0 State: (1, 2) action: D\n",
      "79 0.3333333333333333 new_v -0.36452187229027533 V: 0 Reward(r): -1 State: (1, 2) action: R\n",
      "80 0.5 new_v -0.00900995762420105 V: -0.02002212805378011 Reward(r): 0 State: (0, 0) action: D\n",
      "80 0.5 new_v 0.05737206821541392 V: 0.14751561297692214 Reward(r): 0 State: (0, 0) action: R\n",
      "82 0.5 new_v -0.04655214558852486 V: -0.10344921241894413 Reward(r): 0 State: (2, 1) action: L\n",
      "82 0.5 new_v -0.21432054946611523 V: -0.37281867528353413 Reward(r): 0 State: (2, 1) action: R\n",
      "83 0.5 new_v -0.00900995762420105 V: -0.02002212805378011 Reward(r): 0 State: (2, 0) action: U\n",
      "83 0.5 new_v -0.1054542048839529 V: -0.21432054946611523 Reward(r): 0 State: (2, 0) action: R\n",
      "84 0.5 new_v -0.16776840387759037 V: -0.37281867528353413 Reward(r): 0 State: (2, 3) action: L\n",
      "84 0.5 new_v -0.6677684038775904 V: 0 Reward(r): -1 State: (2, 3) action: U\n",
      "85 0.3333333333333333 new_v -0.06429616483983457 V: -0.21432054946611523 Reward(r): 0 State: (2, 2) action: L\n",
      "85 0.3333333333333333 new_v -0.2646266860031117 V: -0.6677684038775904 Reward(r): 0 State: (2, 2) action: R\n",
      "85 0.3333333333333333 new_v -0.37398324769019425 V: -0.36452187229027533 Reward(r): 0 State: (2, 2) action: U\n",
      "86 0.5 new_v 0.025817430696936264 V: 0.05737206821541392 Reward(r): 0 State: (1, 0) action: U\n",
      "86 0.5 new_v -0.021636961500842548 V: -0.1054542048839529 Reward(r): 0 State: (1, 0) action: D\n",
      "87 0.3333333333333333 new_v 0.04425468389307664 V: 0.14751561297692214 Reward(r): 0 State: (0, 2) action: L\n",
      "87 0.3333333333333333 new_v -0.06510187779400595 V: -0.36452187229027533 Reward(r): 0 State: (0, 2) action: D\n",
      "87 0.3333333333333333 new_v 0.26823145553932737 V: 0 Reward(r): 1 State: (0, 2) action: R\n",
      "89 0.5 new_v 0.025817430696936264 V: 0.05737206821541392 Reward(r): 0 State: (0, 1) action: L\n",
      "89 0.5 new_v 0.14652158568963358 V: 0.26823145553932737 Reward(r): 0 State: (0, 1) action: R\n",
      "90 0.3333333333333333 new_v 0.0804694366617982 V: 0.26823145553932737 Reward(r): 0 State: (1, 2) action: U\n",
      "90 0.3333333333333333 new_v -0.031725537645260066 V: -0.37398324769019425 Reward(r): 0 State: (1, 2) action: D\n",
      "90 0.3333333333333333 new_v -0.36505887097859335 V: 0 Reward(r): -1 State: (1, 2) action: R\n",
      "91 0.5 new_v -0.009736632675379147 V: -0.021636961500842548 Reward(r): 0 State: (0, 0) action: D\n",
      "91 0.5 new_v 0.05619808088495597 V: 0.14652158568963358 Reward(r): 0 State: (0, 0) action: R\n",
      "93 0.5 new_v -0.04745439219777881 V: -0.1054542048839529 Reward(r): 0 State: (2, 1) action: L\n",
      "93 0.5 new_v -0.2157468536583662 V: -0.37398324769019425 Reward(r): 0 State: (2, 1) action: R\n",
      "94 0.5 new_v -0.009736632675379147 V: -0.021636961500842548 Reward(r): 0 State: (2, 0) action: U\n",
      "94 0.5 new_v -0.10682271682164395 V: -0.2157468536583662 Reward(r): 0 State: (2, 0) action: R\n",
      "95 0.5 new_v -0.1682924614605874 V: -0.37398324769019425 Reward(r): 0 State: (2, 3) action: L\n",
      "95 0.5 new_v -0.6682924614605874 V: 0 Reward(r): -1 State: (2, 3) action: U\n",
      "96 0.3333333333333333 new_v -0.06472405609750986 V: -0.2157468536583662 Reward(r): 0 State: (2, 2) action: L\n",
      "96 0.3333333333333333 new_v -0.2652117945356861 V: -0.6682924614605874 Reward(r): 0 State: (2, 2) action: R\n",
      "96 0.3333333333333333 new_v -0.3747294558292641 V: -0.36505887097859335 Reward(r): 0 State: (2, 2) action: U\n",
      "97 0.5 new_v 0.025289136398230185 V: 0.05619808088495597 Reward(r): 0 State: (1, 0) action: U\n",
      "97 0.5 new_v -0.022781086171509594 V: -0.10682271682164395 Reward(r): 0 State: (1, 0) action: D\n",
      "98 0.3333333333333333 new_v 0.043956475706890076 V: 0.14652158568963358 Reward(r): 0 State: (0, 2) action: L\n",
      "98 0.3333333333333333 new_v -0.06556118558668791 V: -0.36505887097859335 Reward(r): 0 State: (0, 2) action: D\n",
      "98 0.3333333333333333 new_v 0.2677721477466454 V: 0 Reward(r): 1 State: (0, 2) action: R\n",
      "100 0.5 new_v 0.025289136398230185 V: 0.05619808088495597 Reward(r): 0 State: (0, 1) action: L\n",
      "100 0.5 new_v 0.1457866028842206 V: 0.2677721477466454 Reward(r): 0 State: (0, 1) action: R\n",
      "101 0.3333333333333333 new_v 0.08033164432399362 V: 0.2677721477466454 Reward(r): 0 State: (1, 2) action: U\n",
      "101 0.3333333333333333 new_v -0.03208719242478561 V: -0.3747294558292641 Reward(r): 0 State: (1, 2) action: D\n",
      "101 0.3333333333333333 new_v -0.3654205257581189 V: 0 Reward(r): -1 State: (1, 2) action: R\n",
      "102 0.5 new_v -0.010251488777179317 V: -0.022781086171509594 Reward(r): 0 State: (0, 0) action: D\n",
      "102 0.5 new_v 0.055352482520719955 V: 0.1457866028842206 Reward(r): 0 State: (0, 0) action: R\n",
      "104 0.5 new_v -0.04807022256973978 V: -0.10682271682164395 Reward(r): 0 State: (2, 1) action: L\n",
      "104 0.5 new_v -0.21669847769290862 V: -0.3747294558292641 Reward(r): 0 State: (2, 1) action: R\n",
      "105 0.5 new_v -0.010251488777179317 V: -0.022781086171509594 Reward(r): 0 State: (2, 0) action: U\n",
      "105 0.5 new_v -0.1077658037389882 V: -0.21669847769290862 Reward(r): 0 State: (2, 0) action: R\n",
      "106 0.5 new_v -0.16862825512316884 V: -0.3747294558292641 Reward(r): 0 State: (2, 3) action: L\n",
      "106 0.5 new_v -0.6686282551231688 V: 0 Reward(r): -1 State: (2, 3) action: U\n",
      "107 0.3333333333333333 new_v -0.06500954330787259 V: -0.21669847769290862 Reward(r): 0 State: (2, 2) action: L\n",
      "107 0.3333333333333333 new_v -0.26559801984482323 V: -0.6686282551231688 Reward(r): 0 State: (2, 2) action: R\n",
      "107 0.3333333333333333 new_v -0.3752241775722589 V: -0.3654205257581189 Reward(r): 0 State: (2, 2) action: U\n",
      "108 0.5 new_v 0.02490861713432398 V: 0.055352482520719955 Reward(r): 0 State: (1, 0) action: U\n",
      "108 0.5 new_v -0.023585994548220707 V: -0.1077658037389882 Reward(r): 0 State: (1, 0) action: D\n",
      "109 0.3333333333333333 new_v 0.04373598086526618 V: 0.1457866028842206 Reward(r): 0 State: (0, 2) action: L\n",
      "109 0.3333333333333333 new_v -0.0658901768621695 V: -0.3654205257581189 Reward(r): 0 State: (0, 2) action: D\n",
      "109 0.3333333333333333 new_v 0.2674431564711638 V: 0 Reward(r): 1 State: (0, 2) action: R\n",
      "values for uniformly random actions:\n",
      "---------------------------\n",
      " 0.06| 0.15| 0.27| 0.00|\n",
      "---------------------------\n",
      "-0.02| 0.00|-0.37| 0.00|\n",
      "---------------------------\n",
      "-0.11|-0.22|-0.38|-0.67|\n",
      "\n",
      "\n",
      "\n",
      "---------------------------\n",
      "  D  |  R  |  R  |     |\n",
      "---------------------------\n",
      "  D  |     |  R  |     |\n",
      "---------------------------\n",
      "  L  |  R  |  R  |  U  |\n",
      "values for fixed policy:\n",
      "---------------------------\n",
      " 0.00| 0.90| 1.00| 0.00|\n",
      "---------------------------\n",
      " 0.00| 0.00|-1.00| 0.00|\n",
      "---------------------------\n",
      " 0.00|-0.81|-0.90|-1.00|\n"
     ]
    }
   ],
   "source": [
    "# https://deeplearningcourses.com/c/artificial-intelligence-reinforcement-learning-in-python\n",
    "# https://www.udemy.com/artificial-intelligence-reinforcement-learning-in-python\n",
    "from __future__ import print_function, division\n",
    "from builtins import range\n",
    "# Note: you may need to update your version of future\n",
    "# sudo pip install -U future\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "SMALL_ENOUGH = 1e-3 # threshold for convergence\n",
    "\n",
    "def print_values(V, g):\n",
    "  for i in range(g.width):\n",
    "    print(\"---------------------------\")\n",
    "    for j in range(g.height):\n",
    "      v = V.get((i,j), 0)\n",
    "      if v >= 0:\n",
    "        print(\" %.2f|\" % v, end=\"\")\n",
    "      else:\n",
    "        print(\"%.2f|\" % v, end=\"\") # -ve sign takes up an extra space\n",
    "    print(\"\")\n",
    "\n",
    "\n",
    "def print_policy(P, g):\n",
    "  for i in range(g.width):\n",
    "    print(\"---------------------------\")\n",
    "    for j in range(g.height):\n",
    "      a = P.get((i,j), ' ')\n",
    "      print(\"  %s  |\" % a, end=\"\")\n",
    "    print(\"\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  # iterative policy evaluation\n",
    "  # given a policy, let's find it's value function V(s)\n",
    "  # we will do this for both a uniform random policy and fixed policy\n",
    "  # NOTE:\n",
    "  # there are 2 sources of randomness\n",
    "  # p(a|s) - deciding what action to take given the state\n",
    "  # p(s',r|s,a) - the next state and reward given your action-state pair\n",
    "  # we are only modeling p(a|s) = uniform\n",
    "  # how would the code change if p(s',r|s,a) is not deterministic?\n",
    "  grid = standard_grid()\n",
    "\n",
    "  # states will be positions (i,j)\n",
    "  # simpler than tic-tac-toe because we only have one \"game piece\"\n",
    "  # that can only be at one position at a time\n",
    "  states = grid.all_states()\n",
    "\n",
    "  ### uniformly random actions ###\n",
    "  # initialize V(s) = 0\n",
    "  V = {}\n",
    "  for s in states:\n",
    "    V[s] = 0\n",
    "  gamma = .9 # discount factor\n",
    "  # repeat until convergence\n",
    "  count = 0\n",
    "  while True:\n",
    "    \n",
    "    biggest_change = 0\n",
    "    for s in states:\n",
    "      old_v = V[s]\n",
    "      count = count + 1\n",
    "      # V(s) only has value if it's not a terminal state\n",
    "      if s in grid.actions:\n",
    "        \n",
    "        new_v = 0 # we will accumulate the answer\n",
    "        p_a = 1.0 / len(grid.actions[s]) # each action has equal probability\n",
    "        for a in grid.actions[s]:\n",
    "          grid.set_state(s)\n",
    "          r = grid.move(a)\n",
    "          new_v += p_a * (r + gamma * V[grid.current_state()])\n",
    "          print(count,p_a,'new_v',new_v,'V:', V[grid.current_state()],'Reward(r):',r,'State:',s,'action:',a)\n",
    "        V[s] = new_v\n",
    "        biggest_change = max(biggest_change, np.abs(old_v - V[s]))\n",
    "\n",
    "    if biggest_change < SMALL_ENOUGH:\n",
    "      break\n",
    "  print(\"values for uniformly random actions:\")\n",
    "  print_values(V, grid)\n",
    "  print(\"\\n\\n\")\n",
    "\n",
    "  ### fixed policy ###\n",
    "  policy = {\n",
    "    (2, 0): 'L',\n",
    "    (1, 0): 'D',\n",
    "    (0, 0): 'D',\n",
    "    (0, 1): 'R',\n",
    "    (0, 2): 'R',\n",
    "    (1, 2): 'R',\n",
    "    (2, 1): 'R',\n",
    "    (2, 2): 'R',\n",
    "    (2, 3): 'U',\n",
    "  }\n",
    "  print_policy(policy, grid)\n",
    "\n",
    "  # initialize V(s) = 0\n",
    "  V = {}\n",
    "  for s in states:\n",
    "    V[s] = 0\n",
    "\n",
    "  # let's see how V(s) changes as we get further away from the reward\n",
    "  gamma = 0.9 # discount factor\n",
    "\n",
    "  # repeat until convergence\n",
    "  while True:\n",
    "    biggest_change = 0\n",
    "    for s in states:\n",
    "      old_v = V[s]\n",
    "\n",
    "      # V(s) only has value if it's not a terminal state\n",
    "      if s in policy:\n",
    "        a = policy[s]\n",
    "        grid.set_state(s)\n",
    "        r = grid.move(a)\n",
    "        V[s] = r + gamma * V[grid.current_state()]\n",
    "        biggest_change = max(biggest_change, np.abs(old_v - V[s]))\n",
    "\n",
    "    if biggest_change < SMALL_ENOUGH:\n",
    "      break\n",
    "  print(\"values for fixed policy:\")\n",
    "  print_values(V, grid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
